{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:30, 5.64MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n",
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 15:\n",
      "Image - Min Value: 1 Max Value: 225\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAGuRJREFUeJzt3cuzHPd1H/DTPY/7xAVAAARJgRQlUVIk6+EoVpwqJRVv\nsspfm00W8cb2JrLssiNFlGxJFPgwSfEBEsDFxX3MTHcW2qS8O6euxPKpz2d/6sz8pru/06vvMM9z\nAAA9jV/0BwAA/nAEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBoTNADQGOCHgAaE/QA0JigB4DGll/0B/hDeeObe3Nlbo4pPbNeLyqrYv9gLz0zbUtfK+Jy\nW5urWBXP42g/PXOwnz/DP7Zpyl9TERG73S49s1isSrv294/SM+NQe3xMu+I1XFJ7lxnH/DkuF+vS\nrt02/ztHRGyvNumZYaz9Zstl/t6M4s88zLX7ZRwKu4bCUNTuzaurq9Kuv/zrf6x9yP+PN3oAaEzQ\nA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG2rbXLRa1\nr7bd5RuG5mJL01wYHIbasnFVK0AaC+1OU/XvY2FuKvY6FUurSr/ZvCiefaFpbKpei4Xqr2Gs/dDL\nZa3dsNICWDnDiIjlotKKWDuPapffYpc/j8r1GxFx68Zheub52fPSrm2xoXMc89fVYlG7Fsfitf9F\n+bf1aQGAFEEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY21L\nbeZiVcRymT+S5bL4f6nUdZIvsvj9qmqxSuG71XoiolLvMc2181gWS49KDUbFIpGhUDQTc+1a3BUK\nY1bF675cJFK5sObadV86++I9NhTKWCIixkXhftluSrvu3DxJz9y7lZ+JiHjr4buluXG9Ts8cHByU\ndlWcnp7+0Xb9a97oAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGmvbXreotK5FRAz5RqhhKDbDVeYWte91uL9fmjs6yrc7ffr4s9KuXaGJblE7+hgL\nLYUREXOlia74GSsKJXS/nys0BxaL8mIonn2lBbBYHFi7z4oHMkzV1rtdeqb0zImI08I9/YPvf7e0\n69Fnn5fmnjw7T8/s375V2nX//kvpmZ+/+WZp13XwRg8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYE\nPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGmtbajMWS222u216plwkMuUbNzZXl6Vdbzx4UJq7eStf\n+vDhJx+Xdq331umZ6u88FMuBFkN+bi6U9UREzKU2nFqLyzAs8jNjfiai+gkjCrdLDIXf6/dzf7zz\niF2+nCYiYlz88R7fT0+fpmc+KT4HvvTyK6W5J79+mJ7ZbGr35p0XbqdnXvtSvgjnunijB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxte12l9ysi\nYlForVoUt827/P+s5bAq7fpSoW0pIuLjx6fpmXGRb6GLiFhU/nfmywYjImKcar/ZOBTmCtdURMQ4\n5n/rcjPcLv+95k3xui+2PY6FdrhxqD3ihqnyDlR7b1rOxV+t0Mx3Ubl+I2JTaMr79XsflXZ951vf\nKc3df/E8PXNx8by06+6Ng/zMd79e2nUdvNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6\nAGhM0ANAY4IeABoT9ADQmKAHgMYEPQA01ra9btpuSnNDod1pmGvHOBVavG4f3yjtunvzpDT31jvv\np2eWq1p7XaXWrPJ7RUQM1cbBQtPYotC6FhGxLpzjbltrQttVxqpVecW5MQrNksXmwKHwDrTZFKsU\np11tbi7MjcWGvTF/LV5c1X7oYbFXmvvv/+0v0jM/+8lfl3bNF0/SM7/87bulXdfBGz0ANCboAaAx\nQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxtqc3J8UFpbiyUlpxf\nXpV2DWP++P/Dn/5padd+sWjm6ZPT9MywqhWJRKVQqFhqU+1jqUxWinDKc7XjiHGR/89f7mIpnkel\nBWq5rL3LTFPhd94VWqoiYlcpp4mIxZj/sffXxefilH9WTZva9zp/flGae/lW/rs9vlV7Lv7sH/4u\nPfN2/lF6bbzRA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0Jig\nB4DGBD0ANNa2ve6V+y+W5laFdrI3f/VWadfLL72anvmLH/2X0q733/yH0txyzDfRTcV2ssWY/985\nFmYiIuKP2Cg3LIqVcgWr5ao0t9nkm9emaVvaNddK3mJROMfqm8yL9+6nZ3734UelXdup9ikXq/xv\nvVzvlXYNm/x1v503pV17Q+0CuTo/S8+89fCd0q5Hp+fpmeX+vdKu6+CNHgAaE/QA0JigB4DGBD0A\nNCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA01rbUZjXWSku+XCjD+fDjz0q7fvjD\nP0/PvHDjVmnXO5tdae7Bqw/SM5+9VyuKGMb85TgMtcKYoViGMxRKj6ap2OJScOtW7frYXOU/4yef\nfFraNU+1azEKBUvr5bq06itf+Wp65rNHn5d2Ra37JRbLo/TMPNfOfoir9Mw41EqPVvNlae7nv/hl\neubtz2uHvzzJ58TuqnjdXwNv9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxgQ9ADQm6AGgMUEPAI21ba97/6Nas9a4yzcMfe3Lr5Z2fe/rr6dnDhe1Rqjp8llp7mBZaIeb\na82Bw5C/HA/WtXay/eWqNHd6dpae2RRa1yIiFqv8OW63tTausdACuFjU3hOmoXYe4yL/W49j7TxW\n09P0zF7lXomIy23tWlys9tMze3vF1sbn+YbOcVE7++2zj0tzv3g/f2+OJy+Xdu0q1+L0uLTrOnij\nB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxt\ne92nzy5Kc89OP0jPnOx9Xtr1V//zf+SH/vxPSrvW61qT1PnTj9Ize2Ox1WzKX4439o9Ku+4c5Ju/\nIiKefvIoPbPZOyjtWmzz7XVPnz4p7RqnfGvjUH1PKDR/RUQsDvK/9ThelnbtTt9Lzyynq9KucVm7\nhpeL/G/2+st3S7sufpdv85suay2WlxenpbnHZ/nzXxzfKe2a53wD42L84uLWGz0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaKxtqc04FQeX+f8+Z9tagc6P\nf/az9MznTz4s7frRf/x+ae5b3/lWeuZ3f/fL0q7TKX/251fb0q7xRu3SP9zfS89cXNaujzjMl53M\nc+17PXl8lp7ZP7pZ2rVYD6W51VH+7KdtvnwkIuLhW5+kZy4uaiUu495xae7WIn+/fHm/Vrzz8PJx\neubTbe3enM9r1/DVJn8e622+GCgiIhb5gJmidh7XwRs9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAa\nE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY33b62pFUrHZFQbXtYas4fgwPfP2Z09Ku+59\n8Glp7k++mW+ve/WVfNNVRMS7j56lZ84uzku7Pvx0U5q79+Ld9MzwJP+9IiI2i/x1tVysS7sef55v\n2DuearteuHGjNLfYX6VnpmfF9rqH+fvl5v1bpV337uRbCiMiXlrnH9+n7/ymtOvDt9/L76q2Gy73\nS3NjoXk05lpQDFGoR62G0jXwRg8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCN\nCXoAaEzQA0Bjgh4AGmtbahPjUBrb7LbpmWHOl21EROyt9tIzF9vnpV0/+fk/l+aGveP0zJdee1Da\nNR7mC3tOn9fO49njR6W5f/ndR+mZW7dfKu26/cLt/NBcK3HZW3+entlc1Uo6Vsva/TLM+Xt6jPz9\nHBFxuM5/tx98J18AFRHx/tPL0tw//fLN9MzxZW3XvM4XcC1OXijtmpa10qNhLjy7h9o1XImXZeF5\nf1280QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9\nADTWtr1uXtZavObdlJ6ZtrUGpHmTr0Caiz/ZxTb/vSIifv7rt9Iz3/5maVUcHeZbze7evlfaNbz2\ncmnu+UV+ZhxqbVxXQ76N6+piU9pVab1bropNeXsHpbnVsE7PjHOtpfD1V/Of8d1f5e+ViIj/9ZOf\nlebWhde0H3z7+6Vd54XHx7g6Ke2KZTGWNrv8TOF5HxExFxpLp0L74nXxRg8AjQl6AGhM0ANAY4Ie\nABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGutbajPUimbGRaFoZq4VI2w2V+mZ\nYVUrRlgUiyKePT9Pz/z0F2+Wdj14KV80s7faL+16/PSsNHf//ivpmVcf3Cnt+uzp5+mZtx/+qrTr\n7OxxeubL914v7TouFk4dr/NFIsvb+ZmIiIc//Sg985t3flPa9eBLtdKjrz54PT1zdnVY2vXZ6fP0\nzEmxX+lgVXv/3C3zz8Zpqj27x8LcUMyJ6+CNHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM\n0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoLG27XXLsdaQFYt8690w1BrlKrsiaq18UfyM4yJ/ieyK\nH/H9Tx+lZ44Pa81fVxf5Vr6IiA/+z4/TM3/7t/+7tOuVl19Izyzm2n/3//yfvpGeefFO7ezXY74J\nLSLiZqF47fnZaWnX+/vH6Zk3vv2gtOvff+9+aW4uHOOvPqg9B3ZDvmnz6rL2Ox8UWgojImLOP3i2\n8660alV4xg272q7r4I0eABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxgQ9ADQm6AGgsbbtdWPxL8xymR8slCZFRK1Qbo5aA9IwFH/qIf/lVqtac+C60Fq12eZbtSIi\nhrH2ox0e5ivU9ta1Xfvrk/TMi7fyjXcRES+erNMz33qQn4mI2D7/tDS3inzj4LbQvhgRsf/dfJvf\nePJGadeHj94rzb37zvvpmU3cKe1aLaf8zLgp7YriM24qNNHNkf9eERHDUAmYYlBcA2/0ANCYoAeA\nxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxpTb/ynKZL1aplNNE\nRExzvlChMhNRL3GpFUzUDmSet+mZcaxdwttdrTij0kux29Z2nZ0/Tc/c/tqLpV0nB/mZV27fLu3a\nK849vcjfm4c3bpV23bzM/2Y/fadW1vPe4/zvHBFxNu+nZ8ZiscpQeA4MY7UwpjZXuTmnqXZv7goP\n/W2xgOs6eKMHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBorG173cHhujQ3z/lWomJ5XezmfHPSNNfap8ZFsbVqyM+tVrXLaih8xrHw+X6vNrcrtN4t\nFrXzOL04T8/8+r3flHbd/Pob6Zn3P35W2hWXtTtmd/BqembzpNZO9s6nv07PvPXpx6Vdp5fFBsYp\n/4xbRa0ZbrWXbw5cHRUqESPi4OiwNLd9UmmHq10f20Ij5Xa3Ke26Dt7oAaAxQQ8AjQl6AGhM0ANA\nY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0BjbUttbpwcl+bmKV92Mk21oojNNl9y\nMBX/m42L0lip1Ga9zhdgREQsC3Obq1pRRKWcJqJWerS/t1/aNRTmLrbb0q4PPn+anjkofq/lVPvN\nTo7vpme+/Nq3S7u2y9P0zG//5YPSrum89hgex/y1eHRQK5rZO8nPDYcnpV2rca80t3j2JD0zjMWy\nr8Ijf/wCX6u90QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCY\noAeAxgQ9ADTWtr3u6IVas9ZuKrR/bfMtUhER28t8pdx2uijtqqq0+S2K/x+Xy/w5znNtV6GULyIi\njo5upGeW+7Vr8eIy/yHP8iV0ERGxu5e/Fl+6X2uIXG2el+ZeefBGemb/3ldKu26en6VnLs7/sbRr\nGGuP4eUy3/I2rIrPqqt8XdtiuCztulrUGhinyN8vy9W6tGveFNov51rL6XXwRg8AjQl6AGhM0ANA\nY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANBY2/a69UHtq+0K5U7L\nuXiMV/m555e1ZqfLi6vSXKXlbawVZMVcaHdaFBrvIiKGqM0tV6v0zMHxQWnXxdXj9MxcaV+MiKvL\nfBvXtKntevLoUWluF5+mZ378V/9U2nXnxXwz32vf+EZp19sf5L9XRMSq8Pi+2tYa5c6fPkvP7K9r\nzXAHR0elueOj/G92tst/r4iITaHNb7fTXgcA/AEIegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCbo\nAaAxQQ8AjQl6AGhM0ANAY4IeABprW2ozDIvS3Pog/99nNdZ2zVf5YpVNcdc231ny+7kpPzgsi5dV\npUGnMBJRL9652lykZ26s8mUbERGrvfxvXS2aeXae/157xzdLu+6c7JfmzgvX1f/9578v7Xrt+d30\nzFe/8Vpp182L2vVxfpr/zWKuPQjmoVDIUnyN3G1rN/Uw5Rfu7+2Vdm0u8+VAxUi6Ft7oAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGmvbXjcXypYi\nIharQnvdXq0KbVc4/dVUrUCqtTRtV/m2q2lTa5/a7vLNa4txXdpVasqLiGm6Ss+si9fHep2fu1jU\nvtdZoY3raq49Pg5u3yjNvf3uu+mZ1796q7TrhRv5s392+nFp12JVvKeX+ftlKLRRRkQMhZ96rlZE\nFl8/zzfn6Zl1oSEyImJ1kp/brYrncQ280QNAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGhP0ANCYoAeAxtqW2kzFUptpyA/uFrWiiNVhvuRgu62VMIzFgomDg/x/wcuzi9Ku\nZ4W5xVgspVitSnNXm3z5y65QhBMRsSz09az2arf0bshfH/PyoLTr5M7d0tzDv/n79Myzx49Ku/7s\n330vPfPwvVqpzW67X5q7dec4PXP6+Ky0a73J3y/F3qiYlrXn6bTYpGeWB7Wyr5O7+bOfn+ZLiK6L\nN3oAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DG\n2rbX7Yr1daXCpUVt12KZP/55KDY7FeeWy3yz1v5RrY1rs8uf/jjX/qvuHxWq4SIiLvPnOEW+VSsi\nYrXON/MdHNda+cYpf46rvVp73bCoXR83jvNNYz/64bdLu155Id9O9skHn5R2ffzseWnu6G6+BfDq\nsnYtbjb5e3M51O6x/Zu1RrlF4XI8un1Y2nXjhfzc+ukX917tjR4AGhP0ANCYoAeAxgQ9ADQm6AGg\nMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0ANCboAaCxtu11F1eXpbnVeb45adzLt4xFRMyL\nfCPUYlX7yeZ5KM1td/m2q2WhlS8i4vhmvn5qNxVb+W7UGgdP9vOtZgeHtTauzfNtemYqXL8RETcL\nLYVHxafHrcP7pbk/+8F/Tc/M87PSrstn+Sa6W8VGxO/fyrfQRUTcef1r6Znf7P+2tOud9cfpmVsv\n3S7tuvfgpDQ3HeSfVbva7RLLQlPe7U3+2XFdvNEDQGOCHgAaE/QA0JigB4DGBD0ANCboAaAxQQ8A\njQl6AGhM0ANAY4IeABoT9ADQmKAHgMbaltps8/0GERHx/Gm+7GQca8d4GPmimcWq9t9sWevdiWmX\nP49l4XtFROyG/I+2PiqtKl/5+4f5weWqdh5X5/mimeG80LYREfcKpTabXb74JSLi0en7pbmnm8/S\nM2cXH5Z2TfNZfujFfElVRMRrLx6W5m7dPU/PLO+sSrv2nuTbX5a3Sqvi5H7t5rxc5s//bK6Vn1We\nH6vic/E6eKMHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEP\nAI0JegBorG173eHBujQ3FNrhpmIp0WKdP/458m1yERHLRe1D7q3y57i7qlUHDvv573Z8r3j4xSt/\ninyr2d5+7f/0wXgzPbO/vVHadfw83+L1fKw1wz387Glt7vEH6ZlpfVHadXg73wI43q81Bz5e5L9X\nRMTZ9mF6Zrpdq7G8/XL+hnk+PintusoXKUZExHbI32e7+aq0ax7yTXnVltPr4I0eABoT9ADQmKAH\ngMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADTWttTm+G6tvGH/5mF6Zu9m\nrVhl/0a+GGGxOCntiqlWNDPv8qUPB0Ptsloc5s9jeaP2vaZhV5rbzNv0zLBfK84YlvmimcWUn4mI\nuHErP7Nc13adbZ+X5tZ7+bOfCiVVERHTUb5gabdXuxZPh1pRVUT+Gj4fayU/V6tVema3OirtWs6n\npbnKe+t2rD27h8jPbafaM+c6eKMHgMYEPQA0JugBoDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeA\nxgQ9ADQm6AGgMUEPAI0JegBobJjnfGMYAPBvgzd6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugB\noDFBDwCNCXoAaEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA\n0JigB4DGBD0ANCboAaAxQQ8AjQl6AGhM0ANAY4IeABoT9ADQmKAHgMYEPQA0JugBoDFBDwCNCXoA\naEzQA0Bjgh4AGhP0ANCYoAeAxgQ9ADQm6AGgMUEPAI0JegBoTNADQGOCHgAaE/QA0JigB4DGBD0A\nNPb/AG5p/0pjh1CgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0218245160>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)\n",
    "helper.display_stats(cifar10_dataset_folder_path, 2, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_max = x.max()\n",
    "    return x if x_max==0 else x/x_max\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    m = np.zeros(shape=(len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        m[i, x[i]] = 1\n",
    "    return m\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.54901961  0.49019608  0.45098039]\n",
      "  [ 0.57254902  0.50980392  0.47843137]\n",
      "  [ 0.56078431  0.49803922  0.47843137]\n",
      "  ..., \n",
      "  [ 0.66666667  0.56862745  0.51372549]\n",
      "  [ 0.69019608  0.58823529  0.5254902 ]\n",
      "  [ 0.66666667  0.57647059  0.52156863]]\n",
      "\n",
      " [[ 0.4745098   0.42352941  0.50588235]\n",
      "  [ 0.50980392  0.4627451   0.54509804]\n",
      "  [ 0.5254902   0.4745098   0.56078431]\n",
      "  ..., \n",
      "  [ 0.63921569  0.55294118  0.61568627]\n",
      "  [ 0.66666667  0.57254902  0.63137255]\n",
      "  [ 0.66666667  0.58039216  0.63137255]]\n",
      "\n",
      " [[ 0.59607843  0.54509804  0.68235294]\n",
      "  [ 0.61568627  0.56862745  0.70196078]\n",
      "  [ 0.60784314  0.56078431  0.68627451]\n",
      "  ..., \n",
      "  [ 0.69411765  0.60392157  0.75686275]\n",
      "  [ 0.70980392  0.61176471  0.76078431]\n",
      "  [ 0.71764706  0.62745098  0.76078431]]\n",
      "\n",
      " ..., \n",
      " [[ 0.49019608  0.43137255  0.4       ]\n",
      "  [ 0.50588235  0.43921569  0.40392157]\n",
      "  [ 0.29803922  0.2627451   0.18431373]\n",
      "  ..., \n",
      "  [ 0.65882353  0.5372549   0.47058824]\n",
      "  [ 0.61960784  0.49411765  0.40392157]\n",
      "  [ 0.57254902  0.45490196  0.34117647]]\n",
      "\n",
      " [[ 0.33333333  0.30196078  0.2745098 ]\n",
      "  [ 0.36862745  0.31764706  0.27843137]\n",
      "  [ 0.29019608  0.25490196  0.17647059]\n",
      "  ..., \n",
      "  [ 0.63529412  0.51764706  0.41568627]\n",
      "  [ 0.65098039  0.5254902   0.39215686]\n",
      "  [ 0.61960784  0.50196078  0.36078431]]\n",
      "\n",
      " [[ 0.49019608  0.43921569  0.43529412]\n",
      "  [ 0.50980392  0.44313725  0.43529412]\n",
      "  [ 0.41176471  0.35686275  0.29411765]\n",
      "  ..., \n",
      "  [ 0.51764706  0.41568627  0.30588235]\n",
      "  [ 0.50980392  0.39607843  0.25098039]\n",
      "  [ 0.55686275  0.45098039  0.30588235]]]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "(5000, 32, 32, 3)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(valid_features[0])\n",
    "print(valid_labels[0])\n",
    "print(valid_features.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None] + list(image_shape), 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, None, 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    conv2d_shape = conv_ksize + (x_tensor.get_shape().as_list()[-1],) + (conv_num_outputs,)\n",
    "    # print(conv2d_shape)\n",
    "    conv2d_weight = tf.Variable(tf.truncated_normal(conv2d_shape, 0, 0.01))\n",
    "    conv2d_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv2d_strides = (1,) + conv_strides + (1,)\n",
    "    # print(conv2d_strides)\n",
    "    conv2d_padding = 'SAME'\n",
    "    conv2d = tf.nn.conv2d(x_tensor, conv2d_weight, conv2d_strides, conv2d_padding)\n",
    "    conv2d = tf.nn.bias_add(conv2d, conv2d_bias)\n",
    "    relu_activation = tf.nn.relu(conv2d)\n",
    "    max_pool_ksize = (1,) + pool_ksize + (1,)\n",
    "    # print(max_pool_ksize)\n",
    "    max_pool_strides = (1,) + pool_strides + (1,)\n",
    "    # print(max_pool_strides)\n",
    "    max_pool_padding = 'SAME'\n",
    "    max_pool = tf.nn.max_pool(relu_activation, max_pool_ksize, max_pool_strides, max_pool_padding)\n",
    "    return max_pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    flat_shape = [-1, np.prod((x_tensor.get_shape().as_list())[1:])]\n",
    "    # print(x_tensor.get_shape())\n",
    "    # print(flat_shape)\n",
    "    return tf.reshape(x_tensor, flat_shape)  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fc_shape = ((x_tensor.get_shape().as_list())[-1],) + (num_outputs,)\n",
    "    # print(fc_shape)\n",
    "    fc_weight = tf.Variable(tf.truncated_normal(fc_shape, 0, 0.01))\n",
    "    fc_bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fc = tf.add(tf.matmul(x_tensor, fc_weight), fc_bias)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    out_shape = ((x_tensor.get_shape().as_list())[-1],) + (num_outputs,)\n",
    "    # print(out_shape)\n",
    "    out_weight = tf.Variable(tf.truncated_normal(out_shape, 0, 0.001))\n",
    "    out_bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = tf.add(tf.matmul(x_tensor, out_weight), out_bias)\n",
    "    return out   \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 16\n",
    "    conv_ksize=(3,3)\n",
    "    conv_strides=(1,1)\n",
    "    pool_ksize=(4,4)\n",
    "    pool_strides=(1,1)    \n",
    "    model = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    model = flatten(model)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    model = fully_conn(model, 512)\n",
    "    \n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    model = output(model, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss: {} Validation Accuracy: {}'.format(loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "keep_probability = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.1517438888549805 Validation Accuracy: 0.22379998862743378\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.0269737243652344 Validation Accuracy: 0.2833999991416931\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.930446982383728 Validation Accuracy: 0.32099997997283936\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.8305617570877075 Validation Accuracy: 0.3490000069141388\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.7766101360321045 Validation Accuracy: 0.37359997630119324\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.7195004224777222 Validation Accuracy: 0.39979997277259827\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.6621670722961426 Validation Accuracy: 0.4227999746799469\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.623168706893921 Validation Accuracy: 0.43859997391700745\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.5753823518753052 Validation Accuracy: 0.454800009727478\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.53303861618042 Validation Accuracy: 0.4651999771595001\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.4889189004898071 Validation Accuracy: 0.47739994525909424\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.4649274349212646 Validation Accuracy: 0.48659995198249817\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.4347659349441528 Validation Accuracy: 0.4917999804019928\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.4028587341308594 Validation Accuracy: 0.49459996819496155\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.3717658519744873 Validation Accuracy: 0.5037999153137207\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.351217269897461 Validation Accuracy: 0.507599949836731\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.3270922899246216 Validation Accuracy: 0.5125999450683594\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.3291072845458984 Validation Accuracy: 0.5175999402999878\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.2889680862426758 Validation Accuracy: 0.5197999477386475\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.278705358505249 Validation Accuracy: 0.5145999193191528\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.277112364768982 Validation Accuracy: 0.5085999369621277\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.2509195804595947 Validation Accuracy: 0.5101999640464783\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.2292242050170898 Validation Accuracy: 0.5191999673843384\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.2011139392852783 Validation Accuracy: 0.528999924659729\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.1904394626617432 Validation Accuracy: 0.5293999910354614\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.1762186288833618 Validation Accuracy: 0.5303999185562134\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.1556236743927002 Validation Accuracy: 0.5353999137878418\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.1421189308166504 Validation Accuracy: 0.5345999598503113\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.1270036697387695 Validation Accuracy: 0.5425999164581299\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.118356704711914 Validation Accuracy: 0.5375999212265015\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.083357334136963 Validation Accuracy: 0.5473998785018921\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.0693469047546387 Validation Accuracy: 0.5505999326705933\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.0729234218597412 Validation Accuracy: 0.542199969291687\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.055213212966919 Validation Accuracy: 0.5375999212265015\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.0381008386611938 Validation Accuracy: 0.554599940776825\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.0373260974884033 Validation Accuracy: 0.5539999008178711\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.0113605260849 Validation Accuracy: 0.5489999055862427\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.9957549571990967 Validation Accuracy: 0.5585999488830566\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.9911158084869385 Validation Accuracy: 0.5523999333381653\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.9755240082740784 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.9582963585853577 Validation Accuracy: 0.5591999292373657\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.9464308023452759 Validation Accuracy: 0.5567998886108398\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.9429324269294739 Validation Accuracy: 0.5619999170303345\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.9394859671592712 Validation Accuracy: 0.5583999156951904\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.9167845845222473 Validation Accuracy: 0.5659999251365662\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.9109981060028076 Validation Accuracy: 0.5597999691963196\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.8933491706848145 Validation Accuracy: 0.5685999393463135\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.8896137475967407 Validation Accuracy: 0.559999942779541\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.8898205757141113 Validation Accuracy: 0.5523998737335205\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.8587369918823242 Validation Accuracy: 0.5641999840736389\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.8431359529495239 Validation Accuracy: 0.5659999251365662\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.846551239490509 Validation Accuracy: 0.556399941444397\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.8241262435913086 Validation Accuracy: 0.5669999122619629\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.8174750804901123 Validation Accuracy: 0.5655999183654785\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.8058367371559143 Validation Accuracy: 0.5641999244689941\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.8147296905517578 Validation Accuracy: 0.5659999251365662\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.7927041053771973 Validation Accuracy: 0.5649999976158142\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.7863180637359619 Validation Accuracy: 0.5653998851776123\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.788213849067688 Validation Accuracy: 0.5659999251365662\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.7791213393211365 Validation Accuracy: 0.5669999718666077\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.7698948383331299 Validation Accuracy: 0.568399965763092\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.7533910274505615 Validation Accuracy: 0.5735998749732971\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.7538518309593201 Validation Accuracy: 0.5737999677658081\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.7449429631233215 Validation Accuracy: 0.573199987411499\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.7536371946334839 Validation Accuracy: 0.5679998993873596\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.729578971862793 Validation Accuracy: 0.5787999629974365\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.7154379487037659 Validation Accuracy: 0.5685999393463135\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.711732029914856 Validation Accuracy: 0.5749999284744263\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.7296484708786011 Validation Accuracy: 0.5627999305725098\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.6950854659080505 Validation Accuracy: 0.5771999359130859\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.6842448711395264 Validation Accuracy: 0.5749999284744263\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.6812912821769714 Validation Accuracy: 0.5737999677658081\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.6862856149673462 Validation Accuracy: 0.5773999094963074\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.675784170627594 Validation Accuracy: 0.5809999108314514\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.6661593317985535 Validation Accuracy: 0.5729998350143433\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.6546247005462646 Validation Accuracy: 0.5765999555587769\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.6499102115631104 Validation Accuracy: 0.5801998972892761\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.6334449648857117 Validation Accuracy: 0.5799999237060547\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.6410254240036011 Validation Accuracy: 0.572399914264679\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.6503292322158813 Validation Accuracy: 0.5809999704360962\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.632401168346405 Validation Accuracy: 0.574199914932251\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.625630795955658 Validation Accuracy: 0.5787999629974365\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.6070083975791931 Validation Accuracy: 0.5827999114990234\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.5987241268157959 Validation Accuracy: 0.5751999020576477\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.6074635982513428 Validation Accuracy: 0.577799916267395\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.6111664772033691 Validation Accuracy: 0.5773999094963074\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.5911766290664673 Validation Accuracy: 0.5827998518943787\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5944997072219849 Validation Accuracy: 0.5819998979568481\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5936596989631653 Validation Accuracy: 0.5797999501228333\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.5755633115768433 Validation Accuracy: 0.5813999176025391\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.5677810311317444 Validation Accuracy: 0.5787999033927917\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5900112986564636 Validation Accuracy: 0.5793999433517456\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.5748903751373291 Validation Accuracy: 0.5785999298095703\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.5751752853393555 Validation Accuracy: 0.5809999108314514\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.559748113155365 Validation Accuracy: 0.5809999704360962\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.5703157186508179 Validation Accuracy: 0.5765999555587769\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.5441683530807495 Validation Accuracy: 0.5783998966217041\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.5422922372817993 Validation Accuracy: 0.5781999230384827\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.5370311737060547 Validation Accuracy: 0.5791999101638794\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.5726925134658813 Validation Accuracy: 0.5699999332427979\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.5503474473953247 Validation Accuracy: 0.5747999548912048\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.5349542498588562 Validation Accuracy: 0.5761998891830444\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.5060940980911255 Validation Accuracy: 0.5883999466896057\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.5204825401306152 Validation Accuracy: 0.5885998606681824\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.5077673196792603 Validation Accuracy: 0.5903998613357544\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.5071358680725098 Validation Accuracy: 0.5905998945236206\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.49332374334335327 Validation Accuracy: 0.5885999202728271\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.48622792959213257 Validation Accuracy: 0.5849999189376831\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.4883826971054077 Validation Accuracy: 0.5893999338150024\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.47851884365081787 Validation Accuracy: 0.5859999656677246\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.48566704988479614 Validation Accuracy: 0.584399938583374\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.4708642363548279 Validation Accuracy: 0.5884000062942505\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.47554540634155273 Validation Accuracy: 0.584399938583374\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.46467092633247375 Validation Accuracy: 0.5883999466896057\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.47283846139907837 Validation Accuracy: 0.5875999331474304\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.45171207189559937 Validation Accuracy: 0.5931999087333679\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.4468938112258911 Validation Accuracy: 0.5889999866485596\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.4467742443084717 Validation Accuracy: 0.5849999189376831\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.4599965810775757 Validation Accuracy: 0.5867999196052551\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.44446152448654175 Validation Accuracy: 0.5859999656677246\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.4380091726779938 Validation Accuracy: 0.5901999473571777\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.4212759733200073 Validation Accuracy: 0.5879998803138733\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.4340651333332062 Validation Accuracy: 0.5931999683380127\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.4314238131046295 Validation Accuracy: 0.5931999683380127\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.41989779472351074 Validation Accuracy: 0.5965999364852905\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.4274383783340454 Validation Accuracy: 0.5875999331474304\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.42121586203575134 Validation Accuracy: 0.5911999940872192\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.4140908718109131 Validation Accuracy: 0.5935999155044556\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.4491725564002991 Validation Accuracy: 0.5915998816490173\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.42646199464797974 Validation Accuracy: 0.5941999554634094\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.4079631567001343 Validation Accuracy: 0.5887999534606934\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.41235238313674927 Validation Accuracy: 0.5981999039649963\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.4175769090652466 Validation Accuracy: 0.593799889087677\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.4093884527683258 Validation Accuracy: 0.5899999141693115\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.41392186284065247 Validation Accuracy: 0.5903999209403992\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.3959582448005676 Validation Accuracy: 0.5945999026298523\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.3959236741065979 Validation Accuracy: 0.595599889755249\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.3959193527698517 Validation Accuracy: 0.5991998910903931\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.39118194580078125 Validation Accuracy: 0.5937999486923218\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.3979984223842621 Validation Accuracy: 0.5917999744415283\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.36330917477607727 Validation Accuracy: 0.5939999222755432\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.37098249793052673 Validation Accuracy: 0.5969999432563782\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.3899831175804138 Validation Accuracy: 0.5969998836517334\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.36650973558425903 Validation Accuracy: 0.5947999358177185\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.3709447979927063 Validation Accuracy: 0.5939999222755432\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.3807404637336731 Validation Accuracy: 0.5991998910903931\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.363206684589386 Validation Accuracy: 0.5927999019622803\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.37591809034347534 Validation Accuracy: 0.5895999073982239\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.3726280927658081 Validation Accuracy: 0.5977998971939087\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.3613120913505554 Validation Accuracy: 0.5963999032974243\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.3670765161514282 Validation Accuracy: 0.5963999032974243\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.3671148419380188 Validation Accuracy: 0.5997998714447021\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.35317641496658325 Validation Accuracy: 0.5951998829841614\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.34300360083580017 Validation Accuracy: 0.5991999506950378\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.3408941328525543 Validation Accuracy: 0.6011999249458313\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.3324984908103943 Validation Accuracy: 0.6025999188423157\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.33394378423690796 Validation Accuracy: 0.6009999513626099\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.33366894721984863 Validation Accuracy: 0.598599910736084\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.33195486664772034 Validation Accuracy: 0.5969999432563782\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.3268676698207855 Validation Accuracy: 0.5973999500274658\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.3245706558227539 Validation Accuracy: 0.5993999242782593\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.317851722240448 Validation Accuracy: 0.5987999439239502\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.30949801206588745 Validation Accuracy: 0.5987998843193054\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.30121901631355286 Validation Accuracy: 0.6037999391555786\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.3072570264339447 Validation Accuracy: 0.5973999500274658\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.3068053722381592 Validation Accuracy: 0.5957999229431152\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.3004850447177887 Validation Accuracy: 0.5967999696731567\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.31894034147262573 Validation Accuracy: 0.602199912071228\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.3038705289363861 Validation Accuracy: 0.5975999236106873\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.29659491777420044 Validation Accuracy: 0.5971999168395996\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.3045557737350464 Validation Accuracy: 0.6011998653411865\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.29704681038856506 Validation Accuracy: 0.5921999216079712\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.2879347801208496 Validation Accuracy: 0.5987999439239502\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.28695982694625854 Validation Accuracy: 0.5963999032974243\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.2822052240371704 Validation Accuracy: 0.5981999039649963\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.28593432903289795 Validation Accuracy: 0.5963999629020691\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.28967493772506714 Validation Accuracy: 0.5943999290466309\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.3031933903694153 Validation Accuracy: 0.5973999500274658\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.2734580934047699 Validation Accuracy: 0.5995998978614807\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.3034980297088623 Validation Accuracy: 0.5981999039649963\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.2807980477809906 Validation Accuracy: 0.595599889755249\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.27490562200546265 Validation Accuracy: 0.6035999059677124\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.28176552057266235 Validation Accuracy: 0.5981999635696411\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.26266995072364807 Validation Accuracy: 0.5987999439239502\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.27952226996421814 Validation Accuracy: 0.593799889087677\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.26566755771636963 Validation Accuracy: 0.6005998849868774\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.2648114264011383 Validation Accuracy: 0.6007999777793884\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.2745377719402313 Validation Accuracy: 0.5991998910903931\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.26320359110832214 Validation Accuracy: 0.6023999452590942\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.25645971298217773 Validation Accuracy: 0.600399911403656\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.2785559892654419 Validation Accuracy: 0.5945999622344971\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.26803863048553467 Validation Accuracy: 0.5969998836517334\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.27080467343330383 Validation Accuracy: 0.6005999445915222\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.2421405166387558 Validation Accuracy: 0.598599910736084\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.2538221478462219 Validation Accuracy: 0.5997999310493469\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.2494167536497116 Validation Accuracy: 0.5963999032974243\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.25239044427871704 Validation Accuracy: 0.6003998517990112\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.2530336380004883 Validation Accuracy: 0.603399932384491\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.24550475180149078 Validation Accuracy: 0.6025999188423157\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.24311062693595886 Validation Accuracy: 0.6043999195098877\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.140263557434082 Validation Accuracy: 0.2491999864578247\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 1.941143274307251 Validation Accuracy: 0.2985999584197998\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.8132158517837524 Validation Accuracy: 0.3255999982357025\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.7163364887237549 Validation Accuracy: 0.37919995188713074\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.7297320365905762 Validation Accuracy: 0.39239996671676636\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.732797622680664 Validation Accuracy: 0.4193999767303467\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.6196218729019165 Validation Accuracy: 0.42239999771118164\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.5076318979263306 Validation Accuracy: 0.4246000051498413\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.47343909740448 Validation Accuracy: 0.44519996643066406\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.5614047050476074 Validation Accuracy: 0.44339996576309204\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.5768828392028809 Validation Accuracy: 0.4593999981880188\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.4744577407836914 Validation Accuracy: 0.47119995951652527\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.3487733602523804 Validation Accuracy: 0.4697999656200409\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.3456205129623413 Validation Accuracy: 0.4885999262332916\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.4342297315597534 Validation Accuracy: 0.49059996008872986\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.4760743379592896 Validation Accuracy: 0.4989999532699585\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.3644274473190308 Validation Accuracy: 0.4975999891757965\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.2601226568222046 Validation Accuracy: 0.4941999614238739\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.2588496208190918 Validation Accuracy: 0.5117999315261841\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.3618710041046143 Validation Accuracy: 0.5109999179840088\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.3964325189590454 Validation Accuracy: 0.5221999883651733\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.2911287546157837 Validation Accuracy: 0.5201999545097351\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.2007925510406494 Validation Accuracy: 0.5283999443054199\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.1963896751403809 Validation Accuracy: 0.5351998805999756\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.2839140892028809 Validation Accuracy: 0.5299999117851257\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.3451404571533203 Validation Accuracy: 0.5323998928070068\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.2558965682983398 Validation Accuracy: 0.5337998867034912\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.1923598051071167 Validation Accuracy: 0.5273998975753784\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.1597970724105835 Validation Accuracy: 0.5327999591827393\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.2390309572219849 Validation Accuracy: 0.5429999828338623\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.3163905143737793 Validation Accuracy: 0.5313999652862549\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.2527321577072144 Validation Accuracy: 0.52239990234375\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.1324118375778198 Validation Accuracy: 0.5481999516487122\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.1250011920928955 Validation Accuracy: 0.5561999082565308\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.2026538848876953 Validation Accuracy: 0.5507999062538147\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.2724462747573853 Validation Accuracy: 0.5491999387741089\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.1894742250442505 Validation Accuracy: 0.5377999544143677\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.0875123739242554 Validation Accuracy: 0.5585999488830566\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.0896650552749634 Validation Accuracy: 0.5591999292373657\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.1783726215362549 Validation Accuracy: 0.5559999346733093\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.246417760848999 Validation Accuracy: 0.5639998912811279\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.1612985134124756 Validation Accuracy: 0.5493999123573303\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.075167179107666 Validation Accuracy: 0.5651999711990356\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.076285719871521 Validation Accuracy: 0.5653998851776123\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.1545662879943848 Validation Accuracy: 0.5627999305725098\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.2291631698608398 Validation Accuracy: 0.5683999061584473\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.1125463247299194 Validation Accuracy: 0.5649999380111694\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.0556833744049072 Validation Accuracy: 0.5765999555587769\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.0578837394714355 Validation Accuracy: 0.5751999616622925\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.1314424276351929 Validation Accuracy: 0.5707999467849731\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.2108697891235352 Validation Accuracy: 0.5735999345779419\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.0918354988098145 Validation Accuracy: 0.5725998878479004\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.0325353145599365 Validation Accuracy: 0.5753999352455139\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.014528751373291 Validation Accuracy: 0.5819998979568481\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.1332279443740845 Validation Accuracy: 0.5725998878479004\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.1950973272323608 Validation Accuracy: 0.5751999020576477\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.065153956413269 Validation Accuracy: 0.5725999474525452\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.0234326124191284 Validation Accuracy: 0.5781999230384827\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.0095185041427612 Validation Accuracy: 0.5753999352455139\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.096793532371521 Validation Accuracy: 0.5765998959541321\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.1806193590164185 Validation Accuracy: 0.5767999291419983\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.0479216575622559 Validation Accuracy: 0.5899999141693115\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.0115528106689453 Validation Accuracy: 0.5887998938560486\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.9910833239555359 Validation Accuracy: 0.5929999351501465\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.0657731294631958 Validation Accuracy: 0.5865999460220337\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.1484932899475098 Validation Accuracy: 0.5817998647689819\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.0525237321853638 Validation Accuracy: 0.5771999359130859\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.9942017793655396 Validation Accuracy: 0.5881999731063843\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.9710414409637451 Validation Accuracy: 0.5901999473571777\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.055279016494751 Validation Accuracy: 0.5911998748779297\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.133496880531311 Validation Accuracy: 0.5861998796463013\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.0106565952301025 Validation Accuracy: 0.5965999364852905\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.9731063842773438 Validation Accuracy: 0.5935999155044556\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.9854254722595215 Validation Accuracy: 0.5875998735427856\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.0334564447402954 Validation Accuracy: 0.5971998572349548\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.1003942489624023 Validation Accuracy: 0.5955999493598938\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.0081543922424316 Validation Accuracy: 0.5993999242782593\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.9495250582695007 Validation Accuracy: 0.6023999452590942\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.9434314966201782 Validation Accuracy: 0.5915999412536621\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.0312373638153076 Validation Accuracy: 0.5947999358177185\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.082254409790039 Validation Accuracy: 0.5937999486923218\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.9827345609664917 Validation Accuracy: 0.5947998762130737\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.9508129358291626 Validation Accuracy: 0.5997998118400574\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.9426712989807129 Validation Accuracy: 0.5999999046325684\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.0020192861557007 Validation Accuracy: 0.6041999459266663\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.0828930139541626 Validation Accuracy: 0.602199912071228\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.9796984195709229 Validation Accuracy: 0.5991998910903931\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.9280999302864075 Validation Accuracy: 0.6103999018669128\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.9109908938407898 Validation Accuracy: 0.6033998727798462\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.9924803376197815 Validation Accuracy: 0.6049998998641968\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.0542855262756348 Validation Accuracy: 0.6049998998641968\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.9799756407737732 Validation Accuracy: 0.6033998727798462\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.9336313009262085 Validation Accuracy: 0.6097999215126038\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.9119014739990234 Validation Accuracy: 0.6085999011993408\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.991918683052063 Validation Accuracy: 0.608599841594696\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.0363349914550781 Validation Accuracy: 0.6059999465942383\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.9340837001800537 Validation Accuracy: 0.606999933719635\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.9250792264938354 Validation Accuracy: 0.6099998950958252\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.9094537496566772 Validation Accuracy: 0.607999861240387\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.9755382537841797 Validation Accuracy: 0.6159999370574951\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.033901333808899 Validation Accuracy: 0.6125999093055725\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.9499734044075012 Validation Accuracy: 0.6051998734474182\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.9102051258087158 Validation Accuracy: 0.6109998822212219\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.9031274318695068 Validation Accuracy: 0.6103999018669128\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.9607433676719666 Validation Accuracy: 0.6107999086380005\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0436421632766724 Validation Accuracy: 0.5987999439239502\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.9473798871040344 Validation Accuracy: 0.6171998977661133\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.9081215858459473 Validation Accuracy: 0.60999995470047\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.8792541027069092 Validation Accuracy: 0.6153998970985413\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.9357385039329529 Validation Accuracy: 0.6143999099731445\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.0030773878097534 Validation Accuracy: 0.6121999025344849\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.9261460304260254 Validation Accuracy: 0.6125999093055725\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.8832902908325195 Validation Accuracy: 0.612799882888794\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.8733425736427307 Validation Accuracy: 0.6179999113082886\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.9433934688568115 Validation Accuracy: 0.6167998909950256\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.9910405278205872 Validation Accuracy: 0.6153998970985413\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.9087124466896057 Validation Accuracy: 0.6081998944282532\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.8807327747344971 Validation Accuracy: 0.6189998984336853\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.8588889837265015 Validation Accuracy: 0.6227998733520508\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.9238340854644775 Validation Accuracy: 0.621999979019165\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.9775521755218506 Validation Accuracy: 0.6153998970985413\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.884886622428894 Validation Accuracy: 0.621799886226654\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.8619022965431213 Validation Accuracy: 0.614599883556366\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.8929967284202576 Validation Accuracy: 0.6117998957633972\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.918755829334259 Validation Accuracy: 0.6209999322891235\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.9773579239845276 Validation Accuracy: 0.6189999580383301\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.8753161430358887 Validation Accuracy: 0.6269999146461487\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.864888072013855 Validation Accuracy: 0.6187998652458191\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.8530205488204956 Validation Accuracy: 0.6283999085426331\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.894403338432312 Validation Accuracy: 0.6281999349594116\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.9605975151062012 Validation Accuracy: 0.6299998760223389\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.8814431428909302 Validation Accuracy: 0.6255998611450195\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.8587230443954468 Validation Accuracy: 0.626599907875061\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.8552103638648987 Validation Accuracy: 0.6267998814582825\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.8854143023490906 Validation Accuracy: 0.6263998746871948\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.946095883846283 Validation Accuracy: 0.6149998903274536\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.8544725179672241 Validation Accuracy: 0.6319999098777771\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.8549268245697021 Validation Accuracy: 0.6259998679161072\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.8326351642608643 Validation Accuracy: 0.6301999092102051\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.875654935836792 Validation Accuracy: 0.6317998766899109\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.9371286034584045 Validation Accuracy: 0.6237998604774475\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.8466129899024963 Validation Accuracy: 0.6325998902320862\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.8432704210281372 Validation Accuracy: 0.6321998834609985\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.8224190473556519 Validation Accuracy: 0.6327999234199524\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.871087372303009 Validation Accuracy: 0.6329998970031738\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.9340859055519104 Validation Accuracy: 0.6187998652458191\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.8436622023582458 Validation Accuracy: 0.6283999085426331\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.8410242199897766 Validation Accuracy: 0.6283999085426331\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.8124467730522156 Validation Accuracy: 0.6289998888969421\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.8567154407501221 Validation Accuracy: 0.6351999044418335\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.9127506613731384 Validation Accuracy: 0.6323999166488647\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.843691349029541 Validation Accuracy: 0.6367999315261841\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.8132947087287903 Validation Accuracy: 0.6349999308586121\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.8035292029380798 Validation Accuracy: 0.6305999159812927\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.8460020422935486 Validation Accuracy: 0.6337998509407043\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.9149676561355591 Validation Accuracy: 0.6225998401641846\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.8215213418006897 Validation Accuracy: 0.6313998699188232\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.818467378616333 Validation Accuracy: 0.6251999139785767\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.8057245016098022 Validation Accuracy: 0.634199857711792\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.8489058017730713 Validation Accuracy: 0.6323999166488647\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.8973627090454102 Validation Accuracy: 0.629599928855896\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.8127753138542175 Validation Accuracy: 0.6393998861312866\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.8105738162994385 Validation Accuracy: 0.6365998983383179\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.8112314939498901 Validation Accuracy: 0.6347998976707458\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.8343561291694641 Validation Accuracy: 0.6321998834609985\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.886634886264801 Validation Accuracy: 0.6353998780250549\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.8138659596443176 Validation Accuracy: 0.6353998780250549\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.7978353500366211 Validation Accuracy: 0.6375999450683594\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.7794645428657532 Validation Accuracy: 0.6359999179840088\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.8308091163635254 Validation Accuracy: 0.6393998861312866\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.8871124982833862 Validation Accuracy: 0.6365998983383179\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.8021243810653687 Validation Accuracy: 0.6387999057769775\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.8323580622673035 Validation Accuracy: 0.6375999450683594\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.7799869179725647 Validation Accuracy: 0.6365998983383179\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.8156977891921997 Validation Accuracy: 0.6379998326301575\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.8950788378715515 Validation Accuracy: 0.6275998950004578\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.7979652881622314 Validation Accuracy: 0.6395999193191528\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.8082429766654968 Validation Accuracy: 0.6385998725891113\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.7860063910484314 Validation Accuracy: 0.6387999057769775\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.8152462840080261 Validation Accuracy: 0.6457998752593994\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.8720407485961914 Validation Accuracy: 0.6365998983383179\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.7938972115516663 Validation Accuracy: 0.6361998319625854\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.778380274772644 Validation Accuracy: 0.6391998529434204\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.7620413303375244 Validation Accuracy: 0.6431999206542969\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.8191772699356079 Validation Accuracy: 0.6429998874664307\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.8630084991455078 Validation Accuracy: 0.6387999057769775\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.7871772050857544 Validation Accuracy: 0.6427999138832092\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.7855095863342285 Validation Accuracy: 0.6415998935699463\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.7548203468322754 Validation Accuracy: 0.6391999125480652\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.7939004302024841 Validation Accuracy: 0.6419999003410339\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.8516770005226135 Validation Accuracy: 0.6413998603820801\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.7892768383026123 Validation Accuracy: 0.643799901008606\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.7789669036865234 Validation Accuracy: 0.6423998475074768\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.7571938037872314 Validation Accuracy: 0.6401998996734619\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.7916812896728516 Validation Accuracy: 0.6473998427391052\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.8611108660697937 Validation Accuracy: 0.6383998990058899\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.7734274864196777 Validation Accuracy: 0.6445999145507812\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.7706953883171082 Validation Accuracy: 0.6487998962402344\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.7427294254302979 Validation Accuracy: 0.6523998975753784\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.7837190628051758 Validation Accuracy: 0.6471998691558838\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.848416805267334 Validation Accuracy: 0.637799859046936\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.770622730255127 Validation Accuracy: 0.6413998603820801\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.7540581822395325 Validation Accuracy: 0.6451998949050903\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.7335963249206543 Validation Accuracy: 0.6513998508453369\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.7871125340461731 Validation Accuracy: 0.6499998569488525\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.8589113354682922 Validation Accuracy: 0.6375998854637146\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.7915856838226318 Validation Accuracy: 0.6479998826980591\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.7528625726699829 Validation Accuracy: 0.6485998630523682\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.7389398217201233 Validation Accuracy: 0.6525999307632446\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.7727816104888916 Validation Accuracy: 0.6523998975753784\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.8330490589141846 Validation Accuracy: 0.6441998481750488\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.7594956159591675 Validation Accuracy: 0.6475998759269714\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.7333810925483704 Validation Accuracy: 0.6467998623847961\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.7239672541618347 Validation Accuracy: 0.6513999104499817\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.7645483613014221 Validation Accuracy: 0.6529999375343323\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.8222950100898743 Validation Accuracy: 0.6447998285293579\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.7648130059242249 Validation Accuracy: 0.6495998501777649\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.7387050986289978 Validation Accuracy: 0.6483999490737915\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.7282383441925049 Validation Accuracy: 0.6513999104499817\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.7642614245414734 Validation Accuracy: 0.6461999416351318\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.8125063180923462 Validation Accuracy: 0.654999852180481\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.7616487145423889 Validation Accuracy: 0.6563999056816101\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.7350859045982361 Validation Accuracy: 0.649199903011322\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.7187036275863647 Validation Accuracy: 0.6493999361991882\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.7644167542457581 Validation Accuracy: 0.6495999097824097\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.8095378875732422 Validation Accuracy: 0.6449999213218689\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.7420316934585571 Validation Accuracy: 0.6549999117851257\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.7195520401000977 Validation Accuracy: 0.6551998853683472\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.7053588032722473 Validation Accuracy: 0.658599853515625\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.7670896053314209 Validation Accuracy: 0.6509998440742493\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.7934030294418335 Validation Accuracy: 0.6531999111175537\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.727751612663269 Validation Accuracy: 0.6633999347686768\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.7153716683387756 Validation Accuracy: 0.6571998596191406\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.7189062833786011 Validation Accuracy: 0.6527999043464661\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.7525362372398376 Validation Accuracy: 0.6489999294281006\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.7837463617324829 Validation Accuracy: 0.6483998894691467\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.7236446142196655 Validation Accuracy: 0.6557998657226562\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.7152420282363892 Validation Accuracy: 0.6521998643875122\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.7081681489944458 Validation Accuracy: 0.6511998772621155\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.7473514080047607 Validation Accuracy: 0.6537998914718628\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7703390121459961 Validation Accuracy: 0.6579999327659607\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.725506067276001 Validation Accuracy: 0.6625999212265015\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.7038577795028687 Validation Accuracy: 0.6589999198913574\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.6927002668380737 Validation Accuracy: 0.6571998596191406\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.7414301633834839 Validation Accuracy: 0.6551998853683472\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.782698392868042 Validation Accuracy: 0.6533999443054199\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.7349447011947632 Validation Accuracy: 0.6595998406410217\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.7016202807426453 Validation Accuracy: 0.6607998609542847\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.7050621509552002 Validation Accuracy: 0.6553999185562134\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.7271260023117065 Validation Accuracy: 0.6545999050140381\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.7663023471832275 Validation Accuracy: 0.6659998893737793\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.7235483527183533 Validation Accuracy: 0.663399875164032\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.6995605230331421 Validation Accuracy: 0.6561998128890991\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.6800644397735596 Validation Accuracy: 0.656799852848053\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.7190539836883545 Validation Accuracy: 0.6591999530792236\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.7711318731307983 Validation Accuracy: 0.6559998989105225\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.7397175431251526 Validation Accuracy: 0.6607998609542847\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.7198251485824585 Validation Accuracy: 0.6577998995780945\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.6826137900352478 Validation Accuracy: 0.6541998982429504\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.7079260945320129 Validation Accuracy: 0.6593998670578003\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.7549529671669006 Validation Accuracy: 0.6649999022483826\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.7044035792350769 Validation Accuracy: 0.6617998480796814\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.6974655389785767 Validation Accuracy: 0.6603997945785522\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.6934089064598083 Validation Accuracy: 0.6533998847007751\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.7103317975997925 Validation Accuracy: 0.6581999063491821\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.7468076348304749 Validation Accuracy: 0.658599853515625\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.6969050168991089 Validation Accuracy: 0.6633999347686768\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.6725578904151917 Validation Accuracy: 0.6679998636245728\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.6685643792152405 Validation Accuracy: 0.6633999347686768\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.7113777995109558 Validation Accuracy: 0.6627998352050781\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.7576667070388794 Validation Accuracy: 0.6593998670578003\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.6894702911376953 Validation Accuracy: 0.6631999015808105\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.6772364377975464 Validation Accuracy: 0.6655998229980469\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.6566954851150513 Validation Accuracy: 0.6665998697280884\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.6889386773109436 Validation Accuracy: 0.6655998826026917\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.7332523465156555 Validation Accuracy: 0.6585999131202698\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.6882498264312744 Validation Accuracy: 0.6625999212265015\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.6629889607429504 Validation Accuracy: 0.6637998819351196\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.6500163078308105 Validation Accuracy: 0.663399875164032\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.6895085573196411 Validation Accuracy: 0.6655998826026917\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.7410344481468201 Validation Accuracy: 0.6627998948097229\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.6943280696868896 Validation Accuracy: 0.666999876499176\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.6809934377670288 Validation Accuracy: 0.663399875164032\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.6407260894775391 Validation Accuracy: 0.6665998697280884\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.6733121275901794 Validation Accuracy: 0.6689999103546143\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.7090928554534912 Validation Accuracy: 0.6701998710632324\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.6788975596427917 Validation Accuracy: 0.6653998494148254\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.6455568671226501 Validation Accuracy: 0.6689998507499695\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.6310185194015503 Validation Accuracy: 0.6683999300003052\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.6807731986045837 Validation Accuracy: 0.666999876499176\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.7167528867721558 Validation Accuracy: 0.6617998480796814\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.6730012893676758 Validation Accuracy: 0.6665999293327332\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.650693953037262 Validation Accuracy: 0.6717998385429382\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.6250689625740051 Validation Accuracy: 0.6735998392105103\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.6811467409133911 Validation Accuracy: 0.6675999164581299\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.7161225080490112 Validation Accuracy: 0.6637998819351196\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.6864777207374573 Validation Accuracy: 0.6625998616218567\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.659371018409729 Validation Accuracy: 0.6677998900413513\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.6532837748527527 Validation Accuracy: 0.6609998345375061\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.6616320013999939 Validation Accuracy: 0.6671998500823975\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.7096708416938782 Validation Accuracy: 0.6681998372077942\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.657953679561615 Validation Accuracy: 0.6711999177932739\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.6472744345664978 Validation Accuracy: 0.6735998392105103\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.6368342041969299 Validation Accuracy: 0.6665998697280884\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.6716873645782471 Validation Accuracy: 0.6677998900413513\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.729780912399292 Validation Accuracy: 0.662199854850769\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.6413484811782837 Validation Accuracy: 0.6741998195648193\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.635877788066864 Validation Accuracy: 0.6715998649597168\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.6214697360992432 Validation Accuracy: 0.6669999361038208\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.6661916971206665 Validation Accuracy: 0.6665999293327332\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6875289678573608 Validation Accuracy: 0.6683999300003052\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.6773699522018433 Validation Accuracy: 0.6751998662948608\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.6333595514297485 Validation Accuracy: 0.6735998392105103\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.6163415312767029 Validation Accuracy: 0.6693999171257019\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.6499724984169006 Validation Accuracy: 0.6745998859405518\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.6978306174278259 Validation Accuracy: 0.6699998378753662\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.639324426651001 Validation Accuracy: 0.675399899482727\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.6258620619773865 Validation Accuracy: 0.6705998778343201\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.6049476861953735 Validation Accuracy: 0.6683998703956604\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.6487884521484375 Validation Accuracy: 0.668199896812439\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.6863283514976501 Validation Accuracy: 0.6711998581886292\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.6478383541107178 Validation Accuracy: 0.6703998446464539\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.6209748387336731 Validation Accuracy: 0.6711999177932739\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.602760910987854 Validation Accuracy: 0.6691998243331909\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.6386305093765259 Validation Accuracy: 0.6699999570846558\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.6863081455230713 Validation Accuracy: 0.6751999258995056\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.6391980648040771 Validation Accuracy: 0.6645998954772949\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.6299027800559998 Validation Accuracy: 0.6719998717308044\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.6147472262382507 Validation Accuracy: 0.6683998703956604\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.6495475769042969 Validation Accuracy: 0.6689999103546143\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.6799749732017517 Validation Accuracy: 0.6629999279975891\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.6400378346443176 Validation Accuracy: 0.6761999130249023\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.6191067099571228 Validation Accuracy: 0.6755998134613037\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.5995023846626282 Validation Accuracy: 0.678399920463562\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.6277891993522644 Validation Accuracy: 0.6779999136924744\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.673706591129303 Validation Accuracy: 0.6715999245643616\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.6279405355453491 Validation Accuracy: 0.6801998615264893\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.6158176064491272 Validation Accuracy: 0.6761998534202576\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.5866483449935913 Validation Accuracy: 0.6803998947143555\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.6253049373626709 Validation Accuracy: 0.6757998466491699\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.6665880084037781 Validation Accuracy: 0.6725998520851135\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.6226593852043152 Validation Accuracy: 0.6775998473167419\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.6264253854751587 Validation Accuracy: 0.6779999732971191\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.5840619802474976 Validation Accuracy: 0.6773998737335205\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.630893349647522 Validation Accuracy: 0.6765998601913452\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.6582434773445129 Validation Accuracy: 0.6787998676300049\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.6187294721603394 Validation Accuracy: 0.6755998730659485\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.6108083724975586 Validation Accuracy: 0.6729998588562012\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.5920717120170593 Validation Accuracy: 0.6751999258995056\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.605196475982666 Validation Accuracy: 0.678399920463562\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.642064094543457 Validation Accuracy: 0.6711999177932739\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.6081204414367676 Validation Accuracy: 0.6743998527526855\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.6068633198738098 Validation Accuracy: 0.6785998344421387\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.6020268201828003 Validation Accuracy: 0.6735998392105103\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.5945742726325989 Validation Accuracy: 0.6767998933792114\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.66657954454422 Validation Accuracy: 0.6715998649597168\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.6164734959602356 Validation Accuracy: 0.680199921131134\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.609464168548584 Validation Accuracy: 0.6763998866081238\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.5706078410148621 Validation Accuracy: 0.675399899482727\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.6066539287567139 Validation Accuracy: 0.6751998066902161\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.6550838947296143 Validation Accuracy: 0.669999897480011\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.6234155893325806 Validation Accuracy: 0.6719999313354492\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.5956166982650757 Validation Accuracy: 0.680199921131134\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.5756186842918396 Validation Accuracy: 0.6761999130249023\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.5974323153495789 Validation Accuracy: 0.6773999333381653\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.6448698043823242 Validation Accuracy: 0.6695998311042786\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.6117714643478394 Validation Accuracy: 0.6783998012542725\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.5917983055114746 Validation Accuracy: 0.6705998778343201\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.5576685667037964 Validation Accuracy: 0.6787998676300049\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.597615659236908 Validation Accuracy: 0.6827998757362366\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.6330894827842712 Validation Accuracy: 0.6791998744010925\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.5911511182785034 Validation Accuracy: 0.6739999055862427\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.578591525554657 Validation Accuracy: 0.6807999014854431\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.5644285678863525 Validation Accuracy: 0.6785998344421387\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.5976776480674744 Validation Accuracy: 0.6771999001502991\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.6172613501548767 Validation Accuracy: 0.6787998080253601\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.588334858417511 Validation Accuracy: 0.6771998405456543\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.5784232020378113 Validation Accuracy: 0.6823998689651489\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.5658105611801147 Validation Accuracy: 0.6789999008178711\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.6070500612258911 Validation Accuracy: 0.684199869632721\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.6400872468948364 Validation Accuracy: 0.6773999333381653\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.5955984592437744 Validation Accuracy: 0.6753997802734375\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.5774273872375488 Validation Accuracy: 0.6815998554229736\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.5514600872993469 Validation Accuracy: 0.6805998682975769\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.5795173048973083 Validation Accuracy: 0.6771999001502991\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.6180553436279297 Validation Accuracy: 0.6803998947143555\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.5969711542129517 Validation Accuracy: 0.6845998167991638\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.5862753987312317 Validation Accuracy: 0.6791999340057373\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.5532945394515991 Validation Accuracy: 0.6779999136924744\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.5707080960273743 Validation Accuracy: 0.6755998134613037\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.6333015561103821 Validation Accuracy: 0.6769998669624329\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.5871748328208923 Validation Accuracy: 0.6791998744010925\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.5633702278137207 Validation Accuracy: 0.6831998825073242\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.5554481148719788 Validation Accuracy: 0.6861998438835144\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.5721278190612793 Validation Accuracy: 0.6823999285697937\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.6113407611846924 Validation Accuracy: 0.6839998960494995\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.5807098150253296 Validation Accuracy: 0.6807998418807983\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.5624568462371826 Validation Accuracy: 0.6823998689651489\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.5474974513053894 Validation Accuracy: 0.6899999380111694\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.5748330950737 Validation Accuracy: 0.676399827003479\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.6412123441696167 Validation Accuracy: 0.6689998507499695\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.589680552482605 Validation Accuracy: 0.6785998940467834\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.5572544932365417 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.5631889700889587 Validation Accuracy: 0.682999849319458\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.5813590288162231 Validation Accuracy: 0.6761998534202576\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.6157128810882568 Validation Accuracy: 0.6787999272346497\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.5809394717216492 Validation Accuracy: 0.6833999156951904\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.5744536519050598 Validation Accuracy: 0.6805999279022217\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.5487174987792969 Validation Accuracy: 0.6857998967170715\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.563856303691864 Validation Accuracy: 0.6909998059272766\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.6263854503631592 Validation Accuracy: 0.6767999529838562\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.5682213306427002 Validation Accuracy: 0.6879999041557312\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.5652915239334106 Validation Accuracy: 0.6843998432159424\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.546596884727478 Validation Accuracy: 0.6799998879432678\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.5777164697647095 Validation Accuracy: 0.6823998689651489\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.6181758642196655 Validation Accuracy: 0.6767998933792114\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.5667949914932251 Validation Accuracy: 0.685999870300293\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.5443140864372253 Validation Accuracy: 0.6843999028205872\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.5266222357749939 Validation Accuracy: 0.6889997720718384\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.5407059788703918 Validation Accuracy: 0.6831998825073242\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.6106564998626709 Validation Accuracy: 0.6799998879432678\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.5697186589241028 Validation Accuracy: 0.6837998628616333\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.5391744375228882 Validation Accuracy: 0.679399847984314\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.5283564329147339 Validation Accuracy: 0.6917998790740967\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.561481237411499 Validation Accuracy: 0.6853998899459839\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.6141969561576843 Validation Accuracy: 0.6797999143600464\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.5642004609107971 Validation Accuracy: 0.6843998432159424\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.5608546137809753 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.5265053510665894 Validation Accuracy: 0.6919998526573181\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.552237331867218 Validation Accuracy: 0.6833999156951904\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.6040784120559692 Validation Accuracy: 0.6791998147964478\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.5744823217391968 Validation Accuracy: 0.6853998899459839\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.5419713854789734 Validation Accuracy: 0.6863998770713806\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.5369191765785217 Validation Accuracy: 0.68479984998703\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.5581009387969971 Validation Accuracy: 0.6851998567581177\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5921519994735718 Validation Accuracy: 0.6763998866081238\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.560892641544342 Validation Accuracy: 0.6825999021530151\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.5269004106521606 Validation Accuracy: 0.68479984998703\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.5231857299804688 Validation Accuracy: 0.684199869632721\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.5420688986778259 Validation Accuracy: 0.6839998364448547\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.6030092239379883 Validation Accuracy: 0.672799825668335\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.56231689453125 Validation Accuracy: 0.6901998519897461\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.530936062335968 Validation Accuracy: 0.6885998845100403\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.5263254046440125 Validation Accuracy: 0.682999849319458\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.5537051558494568 Validation Accuracy: 0.686599850654602\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.580296516418457 Validation Accuracy: 0.6807999014854431\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.5553444623947144 Validation Accuracy: 0.6861999034881592\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.5604045987129211 Validation Accuracy: 0.6843997836112976\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.557403028011322 Validation Accuracy: 0.6799998879432678\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.5500521659851074 Validation Accuracy: 0.6883999109268188\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.5762901306152344 Validation Accuracy: 0.6817998886108398\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.5612249374389648 Validation Accuracy: 0.68479984998703\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.5572421550750732 Validation Accuracy: 0.6857998371124268\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.5217925906181335 Validation Accuracy: 0.6867998242378235\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.5423994660377502 Validation Accuracy: 0.6897999048233032\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5839029550552368 Validation Accuracy: 0.6891998648643494\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.5507143139839172 Validation Accuracy: 0.6921998262405396\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.5262579917907715 Validation Accuracy: 0.6907998919487\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.5183690786361694 Validation Accuracy: 0.685999870300293\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.5347303748130798 Validation Accuracy: 0.68479984998703\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.5754382610321045 Validation Accuracy: 0.6829999089241028\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.551889181137085 Validation Accuracy: 0.6893998980522156\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.5363122820854187 Validation Accuracy: 0.6853998303413391\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.5210081338882446 Validation Accuracy: 0.6915999054908752\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.5263386964797974 Validation Accuracy: 0.6921998858451843\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.5769988894462585 Validation Accuracy: 0.6891998648643494\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.5540164709091187 Validation Accuracy: 0.6903998255729675\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.5529088973999023 Validation Accuracy: 0.6905998587608337\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.5191023945808411 Validation Accuracy: 0.689599871635437\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.5265594720840454 Validation Accuracy: 0.693199872970581\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.572272002696991 Validation Accuracy: 0.6901999115943909\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.5471289753913879 Validation Accuracy: 0.6917998194694519\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.5343987941741943 Validation Accuracy: 0.6943998336791992\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.5024014711380005 Validation Accuracy: 0.6951998472213745\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.5247128009796143 Validation Accuracy: 0.6879999041557312\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.5729846358299255 Validation Accuracy: 0.6835999488830566\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.5367200374603271 Validation Accuracy: 0.6975998282432556\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.5273272395133972 Validation Accuracy: 0.6911998987197876\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.5007760524749756 Validation Accuracy: 0.6915998458862305\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.5096522569656372 Validation Accuracy: 0.6845998167991638\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.5776560306549072 Validation Accuracy: 0.6771999597549438\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.5608283877372742 Validation Accuracy: 0.6863998770713806\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.5546821355819702 Validation Accuracy: 0.6897999048233032\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.4803702235221863 Validation Accuracy: 0.6937999129295349\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.5177869200706482 Validation Accuracy: 0.6905999183654785\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.57547926902771 Validation Accuracy: 0.6851998567581177\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.5373460054397583 Validation Accuracy: 0.6915998458862305\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.5220918655395508 Validation Accuracy: 0.6899998188018799\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.5232725739479065 Validation Accuracy: 0.685999870300293\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.5207124352455139 Validation Accuracy: 0.6855998635292053\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.5748515129089355 Validation Accuracy: 0.6863998770713806\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.5427192449569702 Validation Accuracy: 0.6899998784065247\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.5127087831497192 Validation Accuracy: 0.6927998661994934\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.5094510912895203 Validation Accuracy: 0.6893998384475708\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.5037968754768372 Validation Accuracy: 0.6907998919487\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.5547869205474854 Validation Accuracy: 0.6861999034881592\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.5386823415756226 Validation Accuracy: 0.694199800491333\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.5373529195785522 Validation Accuracy: 0.6789998412132263\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.5208845138549805 Validation Accuracy: 0.690599799156189\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.5080329775810242 Validation Accuracy: 0.6901999115943909\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.5881274938583374 Validation Accuracy: 0.6793999075889587\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 0.5270580649375916 Validation Accuracy: 0.6907998323440552\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 0.5215844511985779 Validation Accuracy: 0.695399820804596\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 0.49161842465400696 Validation Accuracy: 0.6939998865127563\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 0.5053977370262146 Validation Accuracy: 0.6889998316764832\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.5499889254570007 Validation Accuracy: 0.6907998919487\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 0.514555811882019 Validation Accuracy: 0.6941999197006226\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 0.5118282437324524 Validation Accuracy: 0.6911998987197876\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 0.4849866032600403 Validation Accuracy: 0.6897998452186584\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 0.5187807679176331 Validation Accuracy: 0.6855998635292053\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.5473936796188354 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 0.524050772190094 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 0.5149763226509094 Validation Accuracy: 0.6883999109268188\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 0.4987575113773346 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 0.5032443404197693 Validation Accuracy: 0.690599799156189\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.5412143468856812 Validation Accuracy: 0.6863998770713806\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 0.5124607682228088 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 0.505159854888916 Validation Accuracy: 0.6929998993873596\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 0.4827764332294464 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 0.4921053647994995 Validation Accuracy: 0.6921998858451843\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.5488560795783997 Validation Accuracy: 0.6899998784065247\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 0.5064035058021545 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 0.5056856870651245 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 0.4938347637653351 Validation Accuracy: 0.6923998594284058\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 0.514837384223938 Validation Accuracy: 0.6895998120307922\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.5526930689811707 Validation Accuracy: 0.6893998980522156\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 0.5097857117652893 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 0.4934097230434418 Validation Accuracy: 0.6947998404502869\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 0.4798615574836731 Validation Accuracy: 0.692399799823761\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 0.47845467925071716 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.5406766533851624 Validation Accuracy: 0.6909998655319214\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 0.5172849297523499 Validation Accuracy: 0.6947998404502869\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 0.5282525420188904 Validation Accuracy: 0.6839998960494995\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 0.4763505160808563 Validation Accuracy: 0.6985998153686523\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 0.48826050758361816 Validation Accuracy: 0.6941998600959778\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.5450634956359863 Validation Accuracy: 0.6861999034881592\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 0.5127800703048706 Validation Accuracy: 0.6939998269081116\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 0.5029261708259583 Validation Accuracy: 0.6897998452186584\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 0.4956139922142029 Validation Accuracy: 0.690599799156189\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 0.49764907360076904 Validation Accuracy: 0.6919997930526733\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.5476168990135193 Validation Accuracy: 0.6839998960494995\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 0.5099388360977173 Validation Accuracy: 0.6903998851776123\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 0.4938454031944275 Validation Accuracy: 0.6861998438835144\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 0.4845714271068573 Validation Accuracy: 0.692599892616272\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 0.49440014362335205 Validation Accuracy: 0.7003998756408691\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.5370919704437256 Validation Accuracy: 0.68479984998703\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 0.5040175318717957 Validation Accuracy: 0.6965998411178589\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 0.5078683495521545 Validation Accuracy: 0.6967999339103699\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 0.5008652806282043 Validation Accuracy: 0.6911998391151428\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 0.5067629814147949 Validation Accuracy: 0.6899998784065247\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.525513768196106 Validation Accuracy: 0.6927998661994934\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 0.4857300817966461 Validation Accuracy: 0.6985998749732971\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 0.4899909496307373 Validation Accuracy: 0.694199800491333\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 0.4785900115966797 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 0.4947829246520996 Validation Accuracy: 0.6933999061584473\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.5292638540267944 Validation Accuracy: 0.6867998242378235\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 0.5134779214859009 Validation Accuracy: 0.6975998282432556\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.47897687554359436 Validation Accuracy: 0.6941998600959778\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 0.4598556160926819 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 0.4814348518848419 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.5437224507331848 Validation Accuracy: 0.6913998126983643\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 0.498324453830719 Validation Accuracy: 0.6933998465538025\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 0.48835471272468567 Validation Accuracy: 0.694199800491333\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 0.4676574468612671 Validation Accuracy: 0.6969997882843018\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 0.4975246787071228 Validation Accuracy: 0.6893998384475708\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.5366686582565308 Validation Accuracy: 0.6853998899459839\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 0.49676477909088135 Validation Accuracy: 0.6959998607635498\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 0.4714132249355316 Validation Accuracy: 0.6957998871803284\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 0.4572006165981293 Validation Accuracy: 0.6985999345779419\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 0.47048652172088623 Validation Accuracy: 0.6903998851776123\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.5047486424446106 Validation Accuracy: 0.6885998249053955\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 0.4895007312297821 Validation Accuracy: 0.6917999386787415\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.4906182885169983 Validation Accuracy: 0.6951998472213745\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 0.4882136881351471 Validation Accuracy: 0.6905998587608337\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 0.47902101278305054 Validation Accuracy: 0.6913998126983643\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.5161728858947754 Validation Accuracy: 0.6857998371124268\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 0.4990246593952179 Validation Accuracy: 0.6969999074935913\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.4908308982849121 Validation Accuracy: 0.6969998478889465\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 0.4701048731803894 Validation Accuracy: 0.6927998661994934\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 0.4673035144805908 Validation Accuracy: 0.696199893951416\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.5049723386764526 Validation Accuracy: 0.6943998336791992\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 0.4914950728416443 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.49327099323272705 Validation Accuracy: 0.695399820804596\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 0.46104511618614197 Validation Accuracy: 0.6975998282432556\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 0.47036290168762207 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.50840824842453 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 0.49393415451049805 Validation Accuracy: 0.6927998661994934\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.46869978308677673 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 0.47735661268234253 Validation Accuracy: 0.6887998580932617\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 0.46570441126823425 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.5031273365020752 Validation Accuracy: 0.6981998085975647\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 0.4923328757286072 Validation Accuracy: 0.6961998343467712\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.4800700545310974 Validation Accuracy: 0.6965998411178589\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 0.45783740282058716 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 0.48725736141204834 Validation Accuracy: 0.6847999095916748\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.5300227403640747 Validation Accuracy: 0.6903998851776123\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 0.49763748049736023 Validation Accuracy: 0.6927998661994934\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.482257604598999 Validation Accuracy: 0.691399872303009\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.46129220724105835 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 0.48086410760879517 Validation Accuracy: 0.693199872970581\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.5148505568504333 Validation Accuracy: 0.6917998790740967\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 0.4923946261405945 Validation Accuracy: 0.6931998133659363\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.48784106969833374 Validation Accuracy: 0.6937998533248901\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 0.4558038115501404 Validation Accuracy: 0.6899999380111694\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.4744541049003601 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.507855236530304 Validation Accuracy: 0.692599892616272\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 0.46261459589004517 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.47537678480148315 Validation Accuracy: 0.697199821472168\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.4478094279766083 Validation Accuracy: 0.6961998343467712\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 0.4586034417152405 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.5087994337081909 Validation Accuracy: 0.6925998330116272\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 0.4707837998867035 Validation Accuracy: 0.6963998079299927\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.46302542090415955 Validation Accuracy: 0.694399893283844\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.44566553831100464 Validation Accuracy: 0.694199800491333\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 0.45590338110923767 Validation Accuracy: 0.6873998641967773\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.4940856099128723 Validation Accuracy: 0.6865997910499573\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 0.4706524908542633 Validation Accuracy: 0.6963999271392822\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.47263073921203613 Validation Accuracy: 0.6941998600959778\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.44469186663627625 Validation Accuracy: 0.6963998079299927\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.45656734704971313 Validation Accuracy: 0.6983999013900757\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.49260327219963074 Validation Accuracy: 0.6999999284744263\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 0.4871460497379303 Validation Accuracy: 0.6967998147010803\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.46349623799324036 Validation Accuracy: 0.6999998092651367\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.4341890513896942 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 0.4412420392036438 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.495151549577713 Validation Accuracy: 0.6953998804092407\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 0.49673888087272644 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.4625281095504761 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.4249553084373474 Validation Accuracy: 0.7037999033927917\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.43382522463798523 Validation Accuracy: 0.701999843120575\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.5072917938232422 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 0.4828251898288727 Validation Accuracy: 0.6929998397827148\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.47775954008102417 Validation Accuracy: 0.7021999359130859\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.4365232586860657 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.4449906051158905 Validation Accuracy: 0.6983997821807861\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.4843141734600067 Validation Accuracy: 0.6983999013900757\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 0.4728702902793884 Validation Accuracy: 0.6889998912811279\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.4633822739124298 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.4336836040019989 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.4478294253349304 Validation Accuracy: 0.6977998614311218\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.4948587417602539 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 0.48020198941230774 Validation Accuracy: 0.692599892616272\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.449977844953537 Validation Accuracy: 0.697199821472168\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.44553321599960327 Validation Accuracy: 0.6929997801780701\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.4381074905395508 Validation Accuracy: 0.700799822807312\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.49869561195373535 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 0.472118079662323 Validation Accuracy: 0.6969997882843018\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.4755987226963043 Validation Accuracy: 0.7001998424530029\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.42463427782058716 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.4466129541397095 Validation Accuracy: 0.6921998858451843\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.47816601395606995 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 0.4653472602367401 Validation Accuracy: 0.6963998675346375\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.4621243476867676 Validation Accuracy: 0.6965997815132141\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.42398637533187866 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.439714252948761 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.49176958203315735 Validation Accuracy: 0.6929998993873596\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.4553414583206177 Validation Accuracy: 0.7051998376846313\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.46355366706848145 Validation Accuracy: 0.7005998492240906\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.45359405875205994 Validation Accuracy: 0.6909998655319214\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.43985652923583984 Validation Accuracy: 0.7005997896194458\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.4930669963359833 Validation Accuracy: 0.691199779510498\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 0.47772836685180664 Validation Accuracy: 0.6921998262405396\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.4729425609111786 Validation Accuracy: 0.6997998952865601\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.4395267367362976 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.4495723843574524 Validation Accuracy: 0.6937999129295349\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.4918444752693176 Validation Accuracy: 0.6983998417854309\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.4597683846950531 Validation Accuracy: 0.6897999048233032\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.46127623319625854 Validation Accuracy: 0.697999894618988\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.41492047905921936 Validation Accuracy: 0.699199914932251\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.45005229115486145 Validation Accuracy: 0.6987998485565186\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.47935086488723755 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 0.4611368179321289 Validation Accuracy: 0.695399820804596\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.4502693712711334 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.4072384238243103 Validation Accuracy: 0.7015998363494873\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.4286652207374573 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.4801532030105591 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.45604079961776733 Validation Accuracy: 0.7049999237060547\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.4755881726741791 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.4281911551952362 Validation Accuracy: 0.702599823474884\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.4459194540977478 Validation Accuracy: 0.7017998695373535\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.482269823551178 Validation Accuracy: 0.6997998952865601\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.43761640787124634 Validation Accuracy: 0.7049998641014099\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.44181305170059204 Validation Accuracy: 0.6981998682022095\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.41817915439605713 Validation Accuracy: 0.7039998769760132\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.4250754117965698 Validation Accuracy: 0.7005998492240906\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.4753378629684448 Validation Accuracy: 0.7033998966217041\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.46028298139572144 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.45552679896354675 Validation Accuracy: 0.7027998566627502\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.4278414249420166 Validation Accuracy: 0.7061998844146729\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.43090489506721497 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.47078073024749756 Validation Accuracy: 0.6985998749732971\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.44948747754096985 Validation Accuracy: 0.7011997699737549\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.4532896876335144 Validation Accuracy: 0.6977999210357666\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.4254855215549469 Validation Accuracy: 0.7043998837471008\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.436989963054657 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.4625582993030548 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.4411257803440094 Validation Accuracy: 0.6997998356819153\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.44728657603263855 Validation Accuracy: 0.700799822807312\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.4224332571029663 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.4175485074520111 Validation Accuracy: 0.6929998993873596\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.47760218381881714 Validation Accuracy: 0.6921998262405396\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.447796493768692 Validation Accuracy: 0.704599916934967\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.4601028561592102 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.39539405703544617 Validation Accuracy: 0.704399824142456\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.41668957471847534 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.47166168689727783 Validation Accuracy: 0.6943998336791992\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.4452703595161438 Validation Accuracy: 0.6957998275756836\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.44596999883651733 Validation Accuracy: 0.7055999040603638\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.40999436378479004 Validation Accuracy: 0.705599844455719\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.41519081592559814 Validation Accuracy: 0.6979998350143433\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.463264524936676 Validation Accuracy: 0.7021998763084412\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.43383073806762695 Validation Accuracy: 0.7021998763084412\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.4237235486507416 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.40494897961616516 Validation Accuracy: 0.7033998370170593\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.4222540557384491 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.4643951654434204 Validation Accuracy: 0.6959998607635498\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.45362913608551025 Validation Accuracy: 0.6937998533248901\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.4443556070327759 Validation Accuracy: 0.7035998702049255\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.4154447615146637 Validation Accuracy: 0.701999843120575\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.4076431393623352 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.4413818120956421 Validation Accuracy: 0.704399824142456\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.4504420757293701 Validation Accuracy: 0.700799822807312\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.45936861634254456 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.40451690554618835 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.4073362350463867 Validation Accuracy: 0.6979998350143433\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.45955783128738403 Validation Accuracy: 0.7017999291419983\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.43372124433517456 Validation Accuracy: 0.6963998675346375\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.4349192976951599 Validation Accuracy: 0.702599823474884\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.39562687277793884 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.4164007306098938 Validation Accuracy: 0.7001998424530029\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.45742443203926086 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.43518146872520447 Validation Accuracy: 0.7005998492240906\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.4287138283252716 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.41505175828933716 Validation Accuracy: 0.7055999040603638\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.4306161105632782 Validation Accuracy: 0.7041998505592346\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.44515401124954224 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.44387951493263245 Validation Accuracy: 0.6955997943878174\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.4320095479488373 Validation Accuracy: 0.7021998167037964\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.4068673253059387 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.41344401240348816 Validation Accuracy: 0.6977999210357666\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.46300604939460754 Validation Accuracy: 0.6957998871803284\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.4530678987503052 Validation Accuracy: 0.6987998485565186\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.4656818211078644 Validation Accuracy: 0.6999998092651367\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.4192419648170471 Validation Accuracy: 0.7049998641014099\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.4054792821407318 Validation Accuracy: 0.7005998492240906\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.44288673996925354 Validation Accuracy: 0.6985998153686523\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.43635129928588867 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.43665850162506104 Validation Accuracy: 0.701999843120575\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.3955211341381073 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.41272297501564026 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.460671067237854 Validation Accuracy: 0.7011998891830444\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 0.42596301436424255 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 0.4311312437057495 Validation Accuracy: 0.7061998248100281\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.4010479152202606 Validation Accuracy: 0.70579993724823\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 0.4028211832046509 Validation Accuracy: 0.703799843788147\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.4427269697189331 Validation Accuracy: 0.697999894618988\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 0.4390875995159149 Validation Accuracy: 0.6995998024940491\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.46879634261131287 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.39173606038093567 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 0.3950205445289612 Validation Accuracy: 0.7061998844146729\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.4453902244567871 Validation Accuracy: 0.7023999094963074\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 0.4203886389732361 Validation Accuracy: 0.7041998505592346\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.4357321858406067 Validation Accuracy: 0.6955998539924622\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.39708757400512695 Validation Accuracy: 0.702799916267395\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 0.40577733516693115 Validation Accuracy: 0.7013998031616211\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.44566434621810913 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 0.42724987864494324 Validation Accuracy: 0.7011998891830444\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.4466764032840729 Validation Accuracy: 0.7059998512268066\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.4065154194831848 Validation Accuracy: 0.7039998769760132\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 0.39633646607398987 Validation Accuracy: 0.7027998566627502\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.4366247057914734 Validation Accuracy: 0.7029998302459717\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 0.4060097336769104 Validation Accuracy: 0.7081998586654663\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.4193786382675171 Validation Accuracy: 0.7007998824119568\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.39047515392303467 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 0.40556156635284424 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.43063119053840637 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 0.40789899230003357 Validation Accuracy: 0.7061998248100281\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 0.4183628261089325 Validation Accuracy: 0.70579993724823\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.3991773724555969 Validation Accuracy: 0.7063997983932495\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 0.40141791105270386 Validation Accuracy: 0.703799843788147\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.4454449415206909 Validation Accuracy: 0.6983998417854309\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 0.4323039650917053 Validation Accuracy: 0.7001999020576477\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.41235050559043884 Validation Accuracy: 0.7057998180389404\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.39084261655807495 Validation Accuracy: 0.7061998844146729\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 0.3963733911514282 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.4465864598751068 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 0.4262915849685669 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.4233703911304474 Validation Accuracy: 0.703799843788147\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.3956685960292816 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 0.40125083923339844 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.4368027448654175 Validation Accuracy: 0.703999936580658\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 0.414934366941452 Validation Accuracy: 0.7057998776435852\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.4282941222190857 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.38153594732284546 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 0.3893696665763855 Validation Accuracy: 0.697999894618988\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.42234984040260315 Validation Accuracy: 0.697999894618988\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 0.415566623210907 Validation Accuracy: 0.7057998180389404\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 0.4202979505062103 Validation Accuracy: 0.6995998620986938\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.3879217803478241 Validation Accuracy: 0.7043998837471008\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 0.3934794068336487 Validation Accuracy: 0.703799843788147\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.42854830622673035 Validation Accuracy: 0.7001998424530029\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 0.3960464298725128 Validation Accuracy: 0.7059998512268066\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.4248533546924591 Validation Accuracy: 0.7053998708724976\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.3758673667907715 Validation Accuracy: 0.7101998329162598\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 0.3967033922672272 Validation Accuracy: 0.7047998309135437\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.41422733664512634 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 0.4059060215950012 Validation Accuracy: 0.7013999223709106\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 0.4121355712413788 Validation Accuracy: 0.6997998952865601\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.38369542360305786 Validation Accuracy: 0.7053998708724976\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 0.3795190453529358 Validation Accuracy: 0.7049998641014099\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.4101719260215759 Validation Accuracy: 0.7015998959541321\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 0.40863409638404846 Validation Accuracy: 0.7041997909545898\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.4091200828552246 Validation Accuracy: 0.7065998315811157\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.37836694717407227 Validation Accuracy: 0.7023998498916626\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 0.4112299382686615 Validation Accuracy: 0.7051998376846313\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.41840824484825134 Validation Accuracy: 0.7005997896194458\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 0.4123852849006653 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.41065794229507446 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.37242549657821655 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 0.3912774324417114 Validation Accuracy: 0.7027998566627502\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.4459799528121948 Validation Accuracy: 0.6943998336791992\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.4298686385154724 Validation Accuracy: 0.7033997774124146\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.43947434425354004 Validation Accuracy: 0.6945998668670654\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.3893265426158905 Validation Accuracy: 0.707399845123291\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 0.3861832916736603 Validation Accuracy: 0.701999843120575\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.4225342571735382 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 0.39778804779052734 Validation Accuracy: 0.7009998559951782\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.4234028458595276 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.4038475751876831 Validation Accuracy: 0.701999843120575\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 0.3988325595855713 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.42011353373527527 Validation Accuracy: 0.7003998756408691\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.40099385380744934 Validation Accuracy: 0.7019999027252197\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.4325403869152069 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.380720853805542 Validation Accuracy: 0.7099997997283936\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 0.3879316747188568 Validation Accuracy: 0.7041997909545898\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.4209684133529663 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.4051463305950165 Validation Accuracy: 0.7005997896194458\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.4066106081008911 Validation Accuracy: 0.707599937915802\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.3864024579524994 Validation Accuracy: 0.703999936580658\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 0.3926849663257599 Validation Accuracy: 0.7051998972892761\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.42668044567108154 Validation Accuracy: 0.6971998810768127\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 0.3915249705314636 Validation Accuracy: 0.7019999027252197\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.40149879455566406 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.39191925525665283 Validation Accuracy: 0.7039998173713684\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 0.3818413019180298 Validation Accuracy: 0.7001998424530029\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.4022969901561737 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.3875390589237213 Validation Accuracy: 0.7011998295783997\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.4043905436992645 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.38657042384147644 Validation Accuracy: 0.7101998925209045\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 0.38417503237724304 Validation Accuracy: 0.7037997841835022\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.4256095290184021 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.41507846117019653 Validation Accuracy: 0.6993998885154724\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.40548989176750183 Validation Accuracy: 0.7057998776435852\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.3797820508480072 Validation Accuracy: 0.7119998931884766\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 0.38821542263031006 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.4055992662906647 Validation Accuracy: 0.7081997990608215\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.39661237597465515 Validation Accuracy: 0.7077998518943787\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.39712029695510864 Validation Accuracy: 0.7051998972892761\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.38970720767974854 Validation Accuracy: 0.7011998295783997\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 0.3774190843105316 Validation Accuracy: 0.7085999250411987\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.4105297923088074 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.39892300963401794 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.3960036039352417 Validation Accuracy: 0.7051998376846313\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.3738015592098236 Validation Accuracy: 0.703799843788147\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 0.38442474603652954 Validation Accuracy: 0.7061998844146729\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.4087856411933899 Validation Accuracy: 0.7043998837471008\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 0.3899795413017273 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.4106152355670929 Validation Accuracy: 0.7107998728752136\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.37836602330207825 Validation Accuracy: 0.7031998038291931\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 0.3881128132343292 Validation Accuracy: 0.6997998952865601\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.4041289687156677 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.38295841217041016 Validation Accuracy: 0.7061998248100281\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.4122053384780884 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.36440780758857727 Validation Accuracy: 0.7071999311447144\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 0.37043604254722595 Validation Accuracy: 0.7105998396873474\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.4135463237762451 Validation Accuracy: 0.7079998254776001\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.3758455812931061 Validation Accuracy: 0.7051998972892761\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.38306477665901184 Validation Accuracy: 0.7081998586654663\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.37293562293052673 Validation Accuracy: 0.7083998322486877\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 0.3664243519306183 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.4065232276916504 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.39146167039871216 Validation Accuracy: 0.7041997909545898\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.40062397718429565 Validation Accuracy: 0.7081998586654663\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.37247803807258606 Validation Accuracy: 0.7107998132705688\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 0.3865751028060913 Validation Accuracy: 0.7047998309135437\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.4153510630130768 Validation Accuracy: 0.6975998878479004\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.39473220705986023 Validation Accuracy: 0.7035998106002808\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.3907535672187805 Validation Accuracy: 0.7079998254776001\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.37363821268081665 Validation Accuracy: 0.7097998857498169\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 0.3790780305862427 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.4133484959602356 Validation Accuracy: 0.6989998817443848\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.391584575176239 Validation Accuracy: 0.7055999040603638\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.39909788966178894 Validation Accuracy: 0.7069997787475586\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.35867807269096375 Validation Accuracy: 0.7091999053955078\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 0.3711685538291931 Validation Accuracy: 0.7053999304771423\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.3954179286956787 Validation Accuracy: 0.7035998702049255\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.3964962959289551 Validation Accuracy: 0.7075998187065125\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.39740419387817383 Validation Accuracy: 0.7047998309135437\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.3581288158893585 Validation Accuracy: 0.7127999067306519\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 0.376660019159317 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.3932661712169647 Validation Accuracy: 0.7013999223709106\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.3941359519958496 Validation Accuracy: 0.7089998722076416\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.37688732147216797 Validation Accuracy: 0.710399866104126\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.3510425388813019 Validation Accuracy: 0.7063997983932495\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 0.3588002026081085 Validation Accuracy: 0.6999998688697815\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.4163554012775421 Validation Accuracy: 0.7031998634338379\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.38690659403800964 Validation Accuracy: 0.705599844455719\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.38738515973091125 Validation Accuracy: 0.7071998119354248\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.3644878566265106 Validation Accuracy: 0.7061998248100281\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 0.3733278512954712 Validation Accuracy: 0.7043998837471008\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.3951645493507385 Validation Accuracy: 0.7021999359130859\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 0.40018680691719055 Validation Accuracy: 0.7029998898506165\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.39818209409713745 Validation Accuracy: 0.7083998322486877\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.3587225675582886 Validation Accuracy: 0.7111998796463013\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 0.37403741478919983 Validation Accuracy: 0.7033998966217041\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.4109541177749634 Validation Accuracy: 0.7033998966217041\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.4099409580230713 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.40649741888046265 Validation Accuracy: 0.7133998870849609\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.3528010845184326 Validation Accuracy: 0.7063999176025391\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 0.36050301790237427 Validation Accuracy: 0.7025998830795288\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.40647944808006287 Validation Accuracy: 0.7033998370170593\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.38064032793045044 Validation Accuracy: 0.7097997665405273\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.39605724811553955 Validation Accuracy: 0.7087998390197754\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.35554030537605286 Validation Accuracy: 0.7091997861862183\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 0.3644191026687622 Validation Accuracy: 0.7087998986244202\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.4097353219985962 Validation Accuracy: 0.7013998031616211\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.37769442796707153 Validation Accuracy: 0.7061998844146729\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.3720734715461731 Validation Accuracy: 0.7083998918533325\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.3517448306083679 Validation Accuracy: 0.7115998268127441\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 0.3750677704811096 Validation Accuracy: 0.704399824142456\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.4200407862663269 Validation Accuracy: 0.7013998627662659\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 0.38340768218040466 Validation Accuracy: 0.7089998722076416\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.3836778998374939 Validation Accuracy: 0.710399866104126\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.35752981901168823 Validation Accuracy: 0.708599865436554\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 0.3799062967300415 Validation Accuracy: 0.7033998370170593\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.4027159512042999 Validation Accuracy: 0.7041999101638794\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 0.38442936539649963 Validation Accuracy: 0.7011998295783997\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.4036184847354889 Validation Accuracy: 0.7087998986244202\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.37328606843948364 Validation Accuracy: 0.7137998938560486\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 0.3684762716293335 Validation Accuracy: 0.7033998370170593\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.40243005752563477 Validation Accuracy: 0.6987998485565186\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 0.3854048550128937 Validation Accuracy: 0.6991998553276062\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.3753967583179474 Validation Accuracy: 0.705599844455719\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.3658236861228943 Validation Accuracy: 0.7051998972892761\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 0.3644886314868927 Validation Accuracy: 0.7067998051643372\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.4099455773830414 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.3752630650997162 Validation Accuracy: 0.702799916267395\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.4178690016269684 Validation Accuracy: 0.7057998180389404\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.3500595986843109 Validation Accuracy: 0.7029998302459717\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 0.36642828583717346 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.4007118344306946 Validation Accuracy: 0.7021998763084412\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.37985584139823914 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.36592453718185425 Validation Accuracy: 0.7095998525619507\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.35080474615097046 Validation Accuracy: 0.7107998728752136\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 0.35261332988739014 Validation Accuracy: 0.7065998911857605\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.4184724688529968 Validation Accuracy: 0.6985998153686523\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.38508567214012146 Validation Accuracy: 0.7053998708724976\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.37305447459220886 Validation Accuracy: 0.7063998579978943\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.35230496525764465 Validation Accuracy: 0.7085998058319092\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 0.35159066319465637 Validation Accuracy: 0.7035998702049255\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.3832990527153015 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.37117457389831543 Validation Accuracy: 0.7021998763084412\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.3674505054950714 Validation Accuracy: 0.7079998254776001\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.3458839952945709 Validation Accuracy: 0.7057998776435852\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 0.36694809794425964 Validation Accuracy: 0.7005999088287354\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.4011911451816559 Validation Accuracy: 0.7019999027252197\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 0.3853447735309601 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.39768606424331665 Validation Accuracy: 0.7041997909545898\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.3490942716598511 Validation Accuracy: 0.7089998126029968\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 0.3550465404987335 Validation Accuracy: 0.7071998715400696\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.4065842032432556 Validation Accuracy: 0.7061997652053833\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.37972235679626465 Validation Accuracy: 0.7093998789787292\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.3682950735092163 Validation Accuracy: 0.709199845790863\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.3473771810531616 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 0.3537401258945465 Validation Accuracy: 0.7077997922897339\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.39245378971099854 Validation Accuracy: 0.7049999237060547\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.3817636966705322 Validation Accuracy: 0.7049998641014099\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.3804192543029785 Validation Accuracy: 0.702599823474884\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.3457580804824829 Validation Accuracy: 0.7069998383522034\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 0.3569522500038147 Validation Accuracy: 0.7047998309135437\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.3905230760574341 Validation Accuracy: 0.7063998579978943\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.3756687641143799 Validation Accuracy: 0.7087998986244202\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.3788100779056549 Validation Accuracy: 0.7109998464584351\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.3533463478088379 Validation Accuracy: 0.7077997922897339\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 0.35866162180900574 Validation Accuracy: 0.7057998180389404\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.39074259996414185 Validation Accuracy: 0.7079998254776001\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.3680269420146942 Validation Accuracy: 0.7085999250411987\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.3778233826160431 Validation Accuracy: 0.7047998905181885\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.33990421891212463 Validation Accuracy: 0.7075998783111572\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 0.33618995547294617 Validation Accuracy: 0.7059998512268066\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.3717162013053894 Validation Accuracy: 0.7067998647689819\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.37275058031082153 Validation Accuracy: 0.7089998722076416\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.3708844780921936 Validation Accuracy: 0.7077998518943787\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.36875230073928833 Validation Accuracy: 0.7045998573303223\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 0.3522394299507141 Validation Accuracy: 0.6997997760772705\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.39107248187065125 Validation Accuracy: 0.6999999284744263\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.3688410818576813 Validation Accuracy: 0.7099998593330383\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.3793637752532959 Validation Accuracy: 0.7011998295783997\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.34963926672935486 Validation Accuracy: 0.7111998796463013\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 0.35510388016700745 Validation Accuracy: 0.6995998620986938\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7032398909330368\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76cc5da860>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation is that the convolution model is very sensitive to how the weights are initialized, \n",
    "whether in the convolution layer, the fully connected layer, or the output layer. I started with \n",
    "tf.truncated_normal(shape), with default mean 0 and default standard deviation 1.0, \n",
    "and the validation accuracy of the model can barely pass 10%, which is no better than \n",
    "random selection from the 10 output classes. When the standard deviation was changed to 0.1 or 0.01,\n",
    "the validation accuracy of the model picked up to 50-70%, which is acceptable for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
